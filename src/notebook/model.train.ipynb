{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Mini Projects\\Thesis\\model-multitask-learning\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import  BertModel,BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler\n",
    "from torch.utils.data import SequentialSampler\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "MAX_LEN = 256 # Define the maximum length of tokenized texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm feeling happy today!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This movie is amazing and uplifting.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The weather is gloomy and sad.</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The restaurant service was terrible.</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I feel neutral about this book.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The speech was inspiring and motivational.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The internet trolls are spreading toxicity.</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text  emotion  toxicity\n",
       "0                     I'm feeling happy today!        1         0\n",
       "1         This movie is amazing and uplifting.        1         0\n",
       "2               The weather is gloomy and sad.        2         1\n",
       "3         The restaurant service was terrible.        2         1\n",
       "4              I feel neutral about this book.        0         0\n",
       "5   The speech was inspiring and motivational.        1         0\n",
       "6  The internet trolls are spreading toxicity.        2         1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'text': [\n",
    "        \"I'm feeling happy today!\",\n",
    "        \"This movie is amazing and uplifting.\",\n",
    "        \"The weather is gloomy and sad.\",\n",
    "        \"The restaurant service was terrible.\",\n",
    "        \"I feel neutral about this book.\",\n",
    "        \"The speech was inspiring and motivational.\",\n",
    "        \"The internet trolls are spreading toxicity.\"\n",
    "    ],\n",
    "    'emotion': [1, 1, 2, 2, 0, 1, 2],\n",
    "    'toxicity': [0, 0, 1, 1, 0, 0, 1]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': [101, 1045, 1005, 1049, 3110, 3407, 2651, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " {'input_ids': [101, 2023, 3185, 2003, 6429, 1998, 2039, 26644, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " {'input_ids': [101, 1996, 4633, 2003, 24067, 2100, 1998, 6517, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " {'input_ids': [101, 1996, 4825, 2326, 2001, 6659, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " {'input_ids': [101, 1045, 2514, 8699, 2055, 2023, 2338, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " {'input_ids': [101, 1996, 4613, 2001, 18988, 1998, 14354, 2389, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " {'input_ids': [101, 1996, 4274, 27980, 2024, 9359, 22423, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_texts = [tokenizer(text, add_special_tokens=True) for text in df['text']]\n",
    "tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_ids = tokenized_texts\n",
    "toxicity_labels = np.array(df[\"toxicity\"])\n",
    "emotion_labels = np.array(df['emotion'])\n",
    "\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_inputs, test_inputs, train_toxicity_labels, test_toxicity_labels, train_emotion_labels, test_emotion_labels = train_test_split(\n",
    "    input_ids, \n",
    "    toxicity_labels, \n",
    "    emotion_labels, \n",
    "    random_state=42, test_size=0.2)\n",
    "\n",
    "# Create attention masks\n",
    "train_masks = [mask[\"attention_mask\"] for  mask in train_inputs]\n",
    "test_masks = [mask[\"attention_mask\"] for  mask in test_inputs]\n",
    "\n",
    "train_input_ids = [mask[\"input_ids\"] for  mask in train_inputs]\n",
    "test_input_ids = [mask[\"input_ids\"] for  mask in test_inputs]\n",
    "\n",
    "\n",
    "# Pad and truncate the input_ids and attention_mask to a fixed length\n",
    "\n",
    "train_inputs = pad_sequences(train_input_ids, maxlen=MAX_LEN, dtype='long', \n",
    "                             value=0, truncating='post', padding='post')\n",
    "test_inputs = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype='long', \n",
    "                             value=0, truncating='post', padding='post')\n",
    "train_masks = pad_sequences(train_masks, maxlen=MAX_LEN, dtype='long', \n",
    "                             value=0, truncating='post', padding='post')\n",
    "test_masks = pad_sequences(test_masks, maxlen=MAX_LEN, dtype='long', \n",
    "                             value=0, truncating='post', padding='post')\n",
    "\n",
    "#Define Dataloader\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(torch.tensor(train_inputs), torch.tensor(train_masks), \n",
    "                           torch.tensor(train_toxicity_labels), torch.tensor(train_emotion_labels))\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(torch.tensor(test_inputs), torch.tensor(test_masks), \n",
    "                          torch.tensor(test_toxicity_labels), torch.tensor(test_emotion_labels))\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config the model and intantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiTaskModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (toxicity_classifier): LSTM(768, 128, bidirectional=True)\n",
       "  (emotion_classifier): LSTM(768, 128, bidirectional=True)\n",
       "  (toxicity_softmax): Softmax(dim=1)\n",
       "  (emotion_softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiTaskModel()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss: 11.021903991699219\n",
      "Epoch: 1, Step: 0, Loss: 11.389787673950195\n",
      "Epoch: 2, Step: 0, Loss: 11.110596656799316\n",
      "Epoch: 3, Step: 0, Loss: 10.880319595336914\n",
      "Epoch: 4, Step: 0, Loss: 10.860169410705566\n",
      "Epoch: 5, Step: 0, Loss: 10.729597091674805\n",
      "Epoch: 6, Step: 0, Loss: 10.76536750793457\n",
      "Epoch: 7, Step: 0, Loss: 10.663823127746582\n",
      "Epoch: 8, Step: 0, Loss: 10.63259220123291\n",
      "Epoch: 9, Step: 0, Loss: 10.589326858520508\n",
      "Epoch: 10, Step: 0, Loss: 10.412817001342773\n",
      "Epoch: 11, Step: 0, Loss: 10.261810302734375\n",
      "Epoch: 12, Step: 0, Loss: 10.630132675170898\n",
      "Epoch: 13, Step: 0, Loss: 10.528005599975586\n",
      "Epoch: 14, Step: 0, Loss: 10.492459297180176\n",
      "Epoch: 15, Step: 0, Loss: 10.387003898620605\n",
      "Epoch: 16, Step: 0, Loss: 10.423505783081055\n",
      "Epoch: 17, Step: 0, Loss: 10.223806381225586\n",
      "Epoch: 18, Step: 0, Loss: 10.16076374053955\n",
      "Epoch: 19, Step: 0, Loss: 10.309852600097656\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epoch = 20 \n",
    "for epoch in range(epoch):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        toxicity_labels = batch[2].to(device)\n",
    "        emotion_labels = batch[3].to(device)\n",
    "        \n",
    "        toxicity_logits, emotion_logits,  toxicity_probs , emotion_probs = model(input_ids, attention_mask)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        toxicity_loss = criterion(toxicity_logits, toxicity_labels)\n",
    "        emotion_loss = criterion(emotion_logits, emotion_labels)\n",
    "       \n",
    "\n",
    "        loss = toxicity_loss + emotion_loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch: {epoch}, Step: {step}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiTaskModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (toxicity_classifier): LSTM(768, 128, bidirectional=True)\n",
       "  (emotion_classifier): LSTM(768, 128, bidirectional=True)\n",
       "  (toxicity_softmax): Softmax(dim=1)\n",
       "  (emotion_softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
